# 빅데이터 학습 영상(1강)

## Clustering이란?

- 데이터와 원하는 그룹 K가 있을 때 데이터를 K개의 그룹으로 나누는 것을 말한다.
- 새로운 고객이 들어와서 뉴스를 읽었을 때 그 사람이 어떤 그룹에 속할 것이고 그 사람에게 그 그룹이 읽었던 기사를 추천해줄 수 있다.
- 백화점 고객을 할 수 있고, DNA도 할 수 있고, Call center에서 text clustering을 통해 자동응답도 할 수 있다.
- Clustering은 기준에 따라서 그룹을 나눌 수 있는 경우의 수는 무궁무진하다. 그래서 어렵다.
- 그렇다면 얼마나 좋은 cluster인지 따져볼 장치가 필요한데, Cluster가 있으면, 모든 포인트마다 평균이 되는 센터와 빼서 제곱을 하면 데이터가 비슷한 애들과 Cluster가 되었다면 그 값이 작을 것이다. (Partitional Algorithms)

## K-means Clustering

- 처음에 Random하게 Grouping을 하고 K개의 평균점을 찾기 위해 계산한다. 그리고 데이터와 비교를 하면서 그 센터 점을 움직인다. 그리고 Cluster 센터점이 움직이지 않으면 정지한다.
- 처음에 어떻게 나누고 알고리즘을 돌리지가 중요하다. 처음에 나누는 것이 중요하다.
- 단점
  - Cluster 데이터가 작거나 크면 못 찾음
  - Cluster와 엄청 먼 데이터가 있다면 그 평균점이 이상한 곳에 있을 수 있다.
- 이걸 보완하기 위해서 K-Medoids가 있다. 이건 실제 포인트(데이터)를 센터 점으로 고려하고 계산하는 알고리즘이다.

## Hierarchical Clustering

- 바텀 업 방식의 알고리즘
- 모든 포인트가 독립적인 변수다. 그리고 모든 포인트끼리 거리를 구하고 제일 가까운 포인트끼리 Cluster를 하고 또, 반복하고 반복하고 가까운 포인트를 어디로 잡을지에 따라서 답이 달라질 수 있다. (평균으로 할 수 있고고, 최댓값으로 잡을 수도 있고) 

### Centroid-based Clustering Algorithm

- 최소의 거리가 가지는 data들을 merge한다. 그 상태에서 center끼리의 거리를 계산해서 최소가 되는 데이터들을 merge한다.

### Single-Link Clustering Algorithm

- center가 아니라 Data가 독립적인 point가 되어서 거리를 계산한다. 즉 Clustering 내부에 5개의 데이터 중 가장 가까운 친구를 끌어들여서 Cluster가 6개가 된다. 그래서 Data와 무조건 가까운 애들을 구하는 것이다.

### Average-distance Clustering Algorithm

- Distance를 계산할 때, 평균값에서 가장 작은 값을 구하는 것이다.

## DBSCAN Clustering Algorithms

- Density-Based Clustering Algorithms
- 무슨 소린지 모르겠다.
- Epsilon, MinNumPoints의 값을 미리 정의해야 한다.
- 그리고 임의의 점을 하나 뽑고 그 Boundary를 구한다. 그 Boundary 안에 들어오는 점들은 같은 Cluster로 만들어준다. 그리고 거기에 속한 점에서 Boundary를 따져서 처음의 pointer가 속하지만 다른 점들이 안들어오면 Cluster의 절차가 멈춘다. 여기에 속한 점들은 vistied 처리가 되어서 unvisited 점들을 따져본다.

- Epsilon을 fix하고 MinNumpoints를 늘리면 점점 dense한 Cluster를 한다는 뜻이고, 줄이면 Cluster의 아량이 넓어져서 넓게 만들어진다.
- Epsilon을 늘리면 Cluster가 점점 커진다.




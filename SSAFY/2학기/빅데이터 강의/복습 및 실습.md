# 복습 및 실습

- 기본적인 Data Mining을 프로젝트에 접목 할 것이다. 추천 시스템을 통해서

## Big Data 성공 스토리

- 악세사리 가게에서 고객의 동선, 구매 행동을 분석해서 가게 내의 진열을 전략적으로 바꿔서 매출을 올렸다.
- Spotify라고 해서 글로벌 음악 스트리밍 사이트인데, 추천을 제공해서 어떤 그룹의 음악을 좋아한다면, 그 그룹 같은 소속사의 노래를 추천하는 그런 식

## 사용할 Framework

- Weka
  - Data mining tool
  - JAVA
  - classification algorithm 많음
  - 다운 받으면 됨
- Anaconda
  - http://kdd.snu.ac.kr/python/

## Clustering 이란?

- 기준을 설정해놓고 데이터를 그룹화 하는 것이다. K라는 변수를 둬서 K 개로 그루핑을 하는 것이 기본적인 아이디어이다.
- 기준이 달라지면 Cluster가 달라진다.
- 뉴스 1, 2, 3을 cluster1으로 두고 뉴스 4, 5, 6 cluster 2로 두고 새로운 사람이 들어와서 뉴스 5, 6을 봤을 때 뉴스 4를 추천해야겠지?? 그 패턴을 활용해서 cluster를 한다.
- 컴퓨터는 사람이 가지고 있는 직관은 없어서 cluster를 할 때 가능한 모든 분류 방법을 체크해야한다. 근데 이러면 문제가 뭐냐면, 너무 많다. 종류가 너무 많다.
- 그래서 나온 방법이 거리가 가까운 애들끼리 (최소인 애들끼리) 묶으면 cluster가 될 수 있고 cluster center를 잡아도 된다. cluster center는 각 Data들의 중심을 찾아가는 것이다.
- clustering의 기본은 partition을 두는거고 그 partition을 최적으로 두는 것이다.

## K-means Clustering

- 그룹 K개를 설정해주고 맨 첨으로 실행할 때 최초의 Data를 랜덤으로 Partition을 한다. 그 각 Cluster에 대해 center를 잡을 수 있는데 그 center와 가까운 애들을 묶어준다. 그렇게 새로 Cluster가 묶이면 새로 계산된 center로 다시 Cluster를 해준다. 이 것을 반복한다.
- 어떤 Cluster는 점들이 밀접할 수 있고, 듬성듬성 있을 수 있다. K-means Clustering은 순수하게 거리 정보만 이용하다보니까 크기가 큰 Cluster가 잘리기도 한다.
- 얘는 그리고 원형에 가까운 형태들만 잘 찾을 수 있다. 왜냐면 한 점에 대한 똑같은 거리가 원이라서. 원형이 아닌 모양에 대해선 잘 찾지 못한다.
- 또 다른 단점은 outlier가 있다. 데이터가 측정이 잘못되었거나 예외적인 케이스를 잘 잡아내지 못한다. 왜냐면 cluster center가 outlier 때문에 이상한 곳으로 튈 수가 있다.

## Weka 실습

- weka를 실행하고 데이터를 불러와야합니다. 그리고 arff 파일을 열면 http://kdd.snu.ac.kr/python/ label과 분류한 x, y 좌표에 대한 값이 나온다.
- label에는 A, B, C가 있고 갯수 통계 정보도 보여준다. 다른 attribute (x와 y)를 선택하면 역시나 통계 정보를 알려준다.
- 이걸 이용해서 K-means clustering을 할 것인데 위에 cluster를 클릭하면 choose를 고를 수 있다. 거기서 여러가지 알고리즘을 선택할 수 있다.
- 맨 아래에 simple k-means를 클릭하자. 그리고 오른쪽에 클릭하면 parameter를 수정할 수 있는 창이 든다. 그 중에서 numClusters를 바꾸면 된다. 이게 몇 개의 Cluster로 나눌거냐를 말하는 것이다. 그걸 4로 누르고 Start를 누르면 Cluster가 되었다.
- 그래프는 result list에서 우클릭으로 아래에서 두 번째를 하면된다. Visualize를 누르면 된다. 그리고 x를 x (num)과 y를 y(num)으로 바꾸면 자주보던 그림이 나온다.
- K-means clustering은 어떻게 처음에 Partition을 랜덤으로 하는지에 따라서 값이 달라지니 parameter를 바꾸면 뭐 답도 바뀐다.
- 이제 weka에서 어떻게 데이터를 보는지를 설명한다.
  - Data set인 arff을 열어보면 기본적으로 pandas와 비슷하게 생겼다. 처음에 데이터에 레이블이 주어지고 레이블을 잘 맞추는 것을 중요하다.
  - 버섯이라고 할 때 독버섯이다 아니다를 구분하고 어떤 특징에 따라서 이건 독버섯이다 아니다를 Classification이라고 하고
  - Clustering은 이런 특성을 가진 애들이 독버섯일거 같다를 하는 것이다. (소속 집단을 모른다) (그냥 유사한 애들끼리 모으는거다)
- arff 파일 열어서 저장하면 확장자를 바꿀 수 있는데 csv 파일로 바꿀 수 있다. 그래서 표처럼 보여주니까 되게 편하다. 물론 반대도 가능하다.

## Anaconda 실습

- Jupyter notebook을 사용한다.

- pandas는 csv 파일을 읽게 만들어주는 라이브러리이다.

- http://kdd.snu.ac.kr/python/에서 cpu.csv를 다운 받고 불러와보자

  ```python
  import pandas as pd
  df = pd.read_csv("data/cpu.csv")
  df[:5]
  ```

  ```python
  df.values
  ```

  ```python
  X = df.values[:, :-1] # :는 Select all the rows, :-1는 Select all but the last column
  Y = df.values[:, -1] # -1은 Select only the last column
  ```

  ```python
  X = df.values[:, -1]
  Y = df.values[:, -1]
  print(X[:3])
  print(Y[:3])
  ```

  ```python
  a = 1
  b = 2
  print("a : {}, b: {}".format(a, b))
  ```

  ```python
  for x in ('A', 'B', 'C'):
      print(x)
  ```

## Python으로 K-means clustering 해보기

```python
%matplotlib inline # matplot이 파일 형태로 리턴을 해주는데, 이걸 선언하면 jupyter notebook에서 결과를 띄어준다

import matplotlib.pyplot as plt
```

```python
df = pd.read_csv("data/cluster2.csv")
print("Dimensions of the data = {}".format(df.shape))

# Dimensions of the data = (1300, 2)
# 행이 1300개 columns가 2개

df[:5]

X = df.values
X[:5]

# Set the size of the figure
plt.figure(figsize=(5, 5)) # 도화지를 만드는 것

# Plot the data points
plt.scatter(X[:, 0], X[:, 1], s = 4) # s는 점의 크기, Data를 점으로 찍겠다
plt.show()
```

```python
cmap = "tab10"
```

```python
from sklearn.cluster import KMeans # sklearn에는 다양한 머신러닝 알고리즘이 있다.
k_means = KMeans(n_clusters = 4, random_state = 0)
# n_clusters는 K에 해당하는 몇 개의 그룹으로 나눌 것인가이다.
# random_state은 맨 처음에 시작할 때 seed 값을 줘서 다르게 할 수 있는데, 그거를 바꾸는 것이다.
# max_iter cluster가 계속하다보면 같은 값을 반복하는 경우가 있다. 무한루프에 빠지는 것인데, 그걸 제한 해주는 것이다.
```

```python
y_pred = k_means.fit_predict(X)
print(y_pred[:10]) # 10개에 대해서 cluster를 해라는 뜻이다.
```

```python
print(k_means.cluster_centers_) # 4개로 나눴으니 4개의 center가 결과로 뜬다.
plt.figure(figsize=(5, 5))
plt.scatter(X[:, 0], X[:, 1], c= y_pred, s=4, cmap = cmap)
# c= y_pred의 뜻은 각 포인트의 색깔을 y_pred의 index에 나눠서 색칠해주는 것이다.
# s는 점 크기
plt.show()
```

## Hierarchical Clustering

- 정확한 성능을 내려면 어떤 Clustering이 좋은지 알아야한다.
- K-means Clustering처럼 K를 주고 K개로 그루핑하겠다는 것은 똑같다.
  - 다른 점은 K means는 center라는 대표가 있었는데
  - 얘는 시작이 각자의 점이 group 하나라고 생각한다.
  - 그래서 자기랑 가장 가까운 애들이랑 합쳐가면서 원하는 K개 남을 때까지 Cluster를 한다.
- 2개의 Cluster에 대해서 거리를 구해서 가장 최소가 되는 애들을 합친다. 합칠 때 합치는 방법이 최소점의 거리로 따질 수 있고, 평균으로 따지냐, 최댓값으로 따질 것이냐로 나눌 수 있다.
- single link는 가장 가까운 점을 따진다. complete link는 가장 멀리 떨어진 점을 합친다.
- average link, mean-link, centroid-link (중앙점을 따진다)
- centeroid-link와 k-means의 차이는 전자는 하나의 점을 하나의 cluster라고 따지면서 가장 가까운 애들끼리 합치면서 줄여나가는 것이고, k-means는 전체로 cluster를 한 번 따진다. 그래서 그 중에서 제일 center의 최적화를 따지는 것이다.
- Weka로 따져봅시다.

## Weka로 실습

- 알고리즘을 Hierarchical Cluster를 고르고 parameter를 골라서 하자. 4로 하자

이번엔 python으로 해보자

```python
for i, x in enumerate(('A', 'B', 'C')): # iteration index가 i에 저장된다.
    print("i: {}, x: {}".format(i, x))
```

```python
from sklearn.cluster import AgglomerativeClustering

for i, linkage in enumerate(('single', 'complete')):
    clustering = AgglomerativeClustering(
        linkage=linkage, n_clusters=4)
    y_pred = clustering.fit_predict(X)
    plt.figure(i + 1, figsize = (5, 5))
    plt.scatter(X[:, 0], X[:, 1], c = y_pred, s= 4, cmap=cmap)
plt.show()
```



## Practice

앞에서 보여준 python code를 변형하여 distance를 single, complete, average, ward를 사용하고 각 distance에 대해서 cluster 개수를 2부터 4까지 변경하면서 클러스터링을 해보고 각각 결과를 visualization하는 python code를 만들어보자

```python
from sklearn.cluster import AgglomerativeClustering

for i, linkage in enumerate(('single', 'complete', 'average', 'ward')):
    clustering = AgglomerativeClustering(
        linkage=linkage, n_clusters=2)
    y_pred = clustering.fit_predict(X)
    plt.figure(i + 1, figsize = (5, 5))
    plt.scatter(X[:, 0], X[:, 1], c = y_pred, s= 4, cmap=cmap)
plt.show()

for i, linkage in enumerate(('single', 'complete', 'average', 'ward')):
    clustering = AgglomerativeClustering(
        linkage=linkage, n_clusters=3)
    y_pred = clustering.fit_predict(X)
    plt.figure(i + 1, figsize = (5, 5))
    plt.scatter(X[:, 0], X[:, 1], c = y_pred, s= 4, cmap=cmap)
plt.show()

for i, linkage in enumerate(('single', 'complete', 'average', 'ward')):
    clustering = AgglomerativeClustering(
        linkage=linkage, n_clusters=4)
    y_pred = clustering.fit_predict(X)
    plt.figure(i + 1, figsize = (5, 5))
    plt.scatter(X[:, 0], X[:, 1], c = y_pred, s= 4, cmap=cmap)
plt.show()
```

- 2중 포문으로 처리 가능

```python
from sklearn.cluster import AgglomerativeClustering

for i, linkage in enumerate(('single', 'complete', 'average', 'ward')):
    for j, k in enumerate((2,3,4)):
        clustering = AgglomerativeClustering(linkage=linkage, n_clusters=k, affinity='euclidean')
        y_pred = clustering.fit_predict(X)
        plt.figure(i*3 + j, figsize = (5, 5))
        plt.scatter(X[:, 0], X[:, 1], c = y_pred, s= 4, cmap=cmap)
        plt.title('Link : {},k = {}'.format(linkage, k))
        
plt.show()
```



## DBSCAN Clustering

- 쉽게 말해서 인싸를 찾는 것이다. 인싸와 인싸들 모이는 그룹을 모아서 Cluster를 모으는 것이다.
- 내 주변에 비슷한 애들이 얼마나 있냐를 찾는 것이다.
- DBSCAN Algorithm은 2개의 파라미터가 있는데
  - 입실론 : 자기 안에 어느 정도 거리까지 친구라 칠건가
  - MinPrs : 몇 명이 있어야 인싸로 칠건가를 잡아주는것이다.
- core point : 인싸

- Density-reachable : 어떤 점에서 입실론으로 그렸을 때 인싸가 안에 있으면 걔들은 하나의 Cluster로 친다. 다리 건너서 인싸를 알면 Cluster라고 한다.
- Density-connected : 인싸 한명을 공통적으로 알면 같은 Cluster라고 친다. 공통적으로 아는 인싸가 있으면 같은 그룹
- core point를 중심으로 입실론만큼 안에 친구가 있으면 같은 cluster로 친다. 그리고 인싸의 친구들의 친구까지만 같은 cluster로 친다.
- 그래서 아무 관련 없는 아싸 , outlier를 만들 수 있다.
- 단점이라고 친다면 입실론이랑 , 미니멈 포인트에 정말 민감하다.

## DBSCAN 실습

- 실습하기 전에 해야할 것이 있다. Weka에서 tools에 package manager를 클릭하고 설치하자 DBSCAN은 기본으로 설치가 되어 있지 않다.
- dbscan하고 Install하자. 그 다음은 하던대로 하면 된다.
- 그리고 Python으로 해봅시다

```python
from sklearn.cluster import DBSCAN
dbscan = DBSCAN(eps = 0.05, min_samples=20)
y_pred = dbscan.fit_predict(X)
print(y_pred[:10])
```

```python
# 파란 점들이 아싸이다. outlier
for i, (eps, min_samples) in enumerate (((0.05, 20), (0.06, 20), (0.06, 15), (0.06, 6))):
    dbscan = DBSCAN(eps = eps, min_samples = min_samples)
    y_pred = dbscan.fit_predict(X)
    plt.figure(i + 1, figsize =(5, 5))
    plt.scatter(X[:, 0], X[:, 1], cmap=cmap, s= 4, c= y_pred)
    plt.title("eps: {}, min_samples: {}".format(eps, min_samples))
plt.show()
```

## EM Clustering

- 기존의 Clustering 방법과는 완전 다르다. 그래서 어렵다.

- 확률이라는 것을 이제 사용하게 된다. 우리가 보고 잇는 데이터가 어떤 확률로 인해서 만들어졋다.를 기반으로 한다.

- 빨강공 4개 들어 있는 주머니 2개와 파란공 1개와 빨간공 3개가 들어있는 주머니가 1개가 있다고 했을 때 공을 꺼낸다고 하자. 주머니를 선택하고 공을 꺼내고 이 것을 반복한다고 하자.

- 포켓을 선택하고 공을 선택하면서 반복적으로 했을 때 이렇게 만들어진 것이 우리가 보고있는 데이터라고 생각하면 된다.

- 즉 이걸 역으로 데이터를 보고 주머니가 어떻게 구성이 되었을 까를 보는 것이다.

- likelihood가 나오는데, 포켓의 갯수와 공의 갯수를 정했을때 이 데이터가 나올 확률을 알아가는 것이다. 그 확률이 가장 높은 주머니의 구성을 Likelihood라고 한다.

- 우리가 보고 있는 공 조합으로 부터 주머니의 구성이 무엇인가를 알아가는것이 확률 모델의 기본적인 아이디어이다.

- 그래서 어떤 데이터로 가지고 training을 하냐에 따라서 주머니의 구성이 달라진다.

- EM Clustering이 이런 확률이다.

- Known data observations은 뽑혀져 나온 공들이고, Latent variables, Unknown 어쩌구는 모델이 될 가방의 구성을 말한다.

- 가우시안 분포를 알아야한다.

  - Cluster A가 나타내는 봉우리에서 점을 찍을 수 있고, B에서 봉우리를 점을 찍을 수 있다. 기본적으로 평균이랑 표준편차 분산이 주어지게 되고 추가로 weight(A와B 중 어떤 봉우리를 선택할 확률이 얼마냐) 만약 Cluster A가 0.7의 확률로 골라지면 그리고 또 평균에 가까운 점을 찍어가고 반복을 하는 것이다.
  - 즉 아까 가방을 고르고 공을 고른것처럼 봉우리를 고르고 점을 찍는 것이다.

  사실 실제론 점이 찍혀져 있는 것만 보는 것이다. 점이 찍혀져 있는 것만 보고 어떤 봉우리로 구성되어 있을까를 예측하는 것이 EM 알고리즘이다.

- 이걸 계산하기 위해선 수학적으로 복잡한 수식이 많이 나온다. 굳이 다 이해 할 필요가 없다. 우리가 parameter를 제시하고 계산을 하는 건데 언제 확률이 가장 높은지를 미분하면서 알아가는 것이다.

- parameter를 찾는 과정에 E-step, M-step이 있는데 굳이 알 필요 없고 결과만 알면 된다. 결과로 나온 식들을 어떻게 코드로 쓸건가를 고민하면 된다.

- EM도 역시나 몇 개의 클러스터를 할지 K개를 주게 된다. EM 클러스터는 정말 cluster를 잘한다. 굉장히 복잡하지만 성능은 좋다.

## 실습

- Weka에서도 Em clustering을 할 수 있다.
- 기본적으로 주어진다.
- python으로 짜보자

```python
from sklearn.mixture import GaussianMixture
em = GaussianMixture(n_components=4 , max_iter = 20, random_state = 0) # max_iter은 반복 횟수 / # components는 cluster 갯수
y_pred = em.fit_predict(X)

print(em.weights_)
# 각 클러스터가 선택될 확률을 말한다.
```

```python
from sklearn.mixture import GaussianMixture
em = GaussianMixture(n_components=4 , max_iter = 20, random_state = 0)
y_pred = em.fit_predict(X)
plt.figure(figsize =(5, 5))
plt.scatter(X[:, 0], X[:, 1], cmap=cmap, s= 4, c= y_pred)
```



## PLSI

- 문서 하나에 중심되는 주제가 있을 텐데, 문서를 이루는 단어 하나하나가 주제를 뜻하고 있다고 가정한다.

- 어떤 사람이 글을 쓰는데, OBAMA라는 토픽을 고르려고한다(obama, samsung, apple 중 obama 쓸 확률이 가장 높다.) 그래서 토픽을 고르고 단어를 고른다. (주머니에서 공을 꺼내는 것과 같다. 주머니 : 토픽, 공 : 단어)

- 오바마라는 주제에서 President, GALAXY, BARACK 등 많은 단어 중 BARACK 단어가 나올 확률이 가장 높고 치자. 그래서 BARACK을 썼다. 이걸 반복하면 된다.

  ```
  Barack Galaxy Excellent
  ```

- 먼저 document에 대한 확률이 있고, 주제를 정할 확률이 있고, 주제하에서 단어를 고를 확률이 있을 것이다. 주머니에서 공을 꺼내는 확률이라고 생각하면 된다.

- 어떻게 돌아가는 지 한번 확인해보자
  - 2 문서 : d1, d2
  - 2 단어 : obama, yuna
  - 2 주제 : z1, z2
  - 라고 가정을 해보자
  - 그리고 주어진 문서엔
    - d1 = obama
    - d2 = yuna
    - 라고 쓰여진 문서가 있다고하자
- 우선 토픽을 2개 선정했기 때문에 p(Obama|z1) = 0.6 , p(Yuna|z1) = 0.4 / p(Obama)|z2 = 0.3, p(Yuna|z2) = 0.7 최초에 이렇게 할당을 하자, 참고로 둘 다 더하면 1이다.
- p(z1|d1) = 0.6 , p(z2|d1) = 0.4 / p(z1|d2) = 0.3, p(z2|d2) =0.7 d1에서 z1이 나올 확률을 따진다.
- 즉 우리가 공을 보고서 주머니에서 빨간공이 다 들어가 있을 확률이 얼마나 될까를 나타내는 것이다.
- 이거 계속해서 뭐 계산 반복하면 대표하는 단어를 나타낼 수 있다
- 사실 뭔 얘긴지 모르겠다.

## TWITOBI

- 관심있는 트윗이나 팔로워를 추천하는 트위토비의 역할이다. 유저가 메시지를 적을 때 엠마왓슨을 팔로우 하고 있었다면 엠마왓슨의 정보를 받아서 영향을 받고 글을 추천받는다.

## Matrix Factorization

- 추천시스템에서 사용할 Matrix Factorization
- collaborative filtering method
  - 자기의 과거 기록을 보는것만이 아니라 자기와 비슷한 사람들간의 유사도를 추천한다. 즉 다른 유저들이 추천에 영향을 끼친다.
  - 사람 1이 1, 2, 3을 샀고 사람 2가 1, 3을 샀다면 사람 2에게 2를 추천할 수 있다.

- 우리가 프로젝트에서 하는 건 Matrix Factorization을 하는 영화들의 유사도를 활용하는 것이 프로젝트 목적이다. PLSI 클러스터링 알고리즘을 통해 영화 줄거리 요약을 통해서 분류를 하는데 분류를 한 결과를 아이템 벡터를 만들 때 활용하겠다를 목적으로 하고있다.

- 결국 중요한 것은 이미 주어진 것을 잘 맞추는 model을 만들었더니 안 본 데이터에 대해서 예측을 하더라.의 개념이다.

  
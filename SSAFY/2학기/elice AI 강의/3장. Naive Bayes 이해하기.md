# 3장. Naive Bayes 이해하기

- 다시 Classification으로 돌아가서 Naive Bayes 분류 방법을 공부합시다.
- 되게 많이 사용하는 모델이기 때문에 공부할 만하다. (결과가 좋진 않다)

## Features

- 성질, 데이터에 대한 detail을 뜻한다. / 꽃에서는 꽃잎, 줄기 등등
- 컴퓨터에게는 어떻게든 숫자로 표현해서 넘겨줘야한다. 그 것을 detail를 features로 했다는 말은 detail을 숫자로 바꿨다는 뜻이다.

## Problems

- Digit Recognition
- Email spam filtering
  - 이메일의 spam을 어떻게 구분할 수 있을까요?

## Simple Classification

- 이 사람이 결혼을 했을까 안했을까??
  - features는 2개이다. 아들을 안고있다. / Fashion이 좋다.
  - 결혼을 했는데 아들처럼 보이는 아이를 안고 있는 확률은 0.8이라고 치자
  - 결혼을 했는데 멋지게 입고다니는 확률 0.1이라고 치자
  - 결혼을 안했을 때도 확률 계산할 수 있고, 아들을 안지 않고 있는 확률도 계산할 수 있다.
  - 이 모든 것을 종합해서 결혼을 했다, 안했다를 구분할 수 있다.
- MNIST에도 적용해보자.
  - 우선 features를 생각해보면, 각각의 픽셀을 features라고 둘 수 있다. 8x8이니까 64개의 features가 있다.

## Spam Filter

- Classification을 할 때 spam 일 때 "Free"라는 단어가 나올 확률, spam이 아닐 때 "Hello" 단어가 나올 확률등을 종합한다.
- Text가 데이터일 경우엔 조금 다르게 할 필요가 있다. 단어의 빈도 수가 중요하다. 어느 단어가 몇 번 나왔는지도 중요하기 때문에다. 또한, 어떤 문맥에서 단어가 나왔는지도 중요하다. (어떤 문맥에서 단어가 나왔는지는 사실 Naive Bayes가 처리 못한다)
- Naive Bayes에서도 overfit이 중요한 문제이다. 만약 내가 학습한 데이터에 대해서 처음 본 것이 나오면 애가 많이 당황해하기 때문이다. 그럼 스팸 처리하다가 처음 보는 단어가 나오면 overfitting이 될 수 있다.
- 얘도 Regulrization을 하듯이 Smoothing을 한다. 

## Smoothing에 대해서도 학습을 해야한다.

- Hyperparameters를 사용한다. 이 전에 가중치 w가 Hyperparameter이다. Smoothing은 K로 잡겠다.

---

- Naive Bayes는 baseline을 잡아줄 수 있다. 예를 들어 스팸메일 detection을 할 때 spam은 전체 비율이 1%로 적으니까 모든 메일을 스팸이 아니다라고 예측을 해서는 안된다.
- 근데 최근엔 다른 것을 쓴다.
- Naive Bayes를 했을 때 에러가 뜨면 어떻게 할까?
  - features를 더 많이 써야한다.
  - Naive Bayes는 flexible 해서 features를 다양하게 고려 가능하다.

## Summary

- 조건부 확률을 사용한 모델인 Naive Bayes
- features들이 독립적이다. 그래서 심플하다.
- Smoothing을 통해서 generalization을 하여 예측을 잘해내는 것이 중요하다.